{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3812182a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\isaac\\anaconda3\\lib\\site-packages (0.47.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: transformers in c:\\users\\isaac\\anaconda3\\lib\\site-packages (4.48.3)\n",
      "Requirement already satisfied: torch in c:\\users\\isaac\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from shap) (1.24.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from shap) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from shap) (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from shap) (1.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from shap) (4.66.4)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from shap) (23.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from shap) (0.59.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from shap) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from numba>=0.54->shap) (0.42.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from pandas->shap) (2023.3.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\isaac\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install shap transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586d7d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isaac\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_fake: (23481, 4)\n",
      "Shape of df_true: (21417, 4)\n",
      "\n",
      "Columns of df_fake: ['title' 'text' 'subject' 'date']\n",
      "Columns of df_true: ['title' 'text' 'subject' 'date']\n",
      "\n",
      "Data types of df_fake:\n",
      " title      object\n",
      "text       object\n",
      "subject    object\n",
      "date       object\n",
      "dtype: object\n",
      "\n",
      "Data types of df_true:\n",
      " title      object\n",
      "text       object\n",
      "subject    object\n",
      "date       object\n",
      "dtype: object\n",
      "\n",
      "Missing values in df_fake:\n",
      " title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in df_true:\n",
      " title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "dtype: int64\n",
      "\n",
      "Average text length in df_fake: 2547.396235254035\n",
      "Average text length in df_true: 2383.278517065882\n",
      "\n",
      "Subject distribution in df_fake:\n",
      " News               9050\n",
      "politics           6841\n",
      "left-news          4459\n",
      "Government News    1570\n",
      "US_News             783\n",
      "Middle-east         778\n",
      "Name: subject, dtype: int64\n",
      "\n",
      "Subject distribution in df_true:\n",
      " politicsNews    11272\n",
      "worldnews       10145\n",
      "Name: subject, dtype: int64\n",
      "\n",
      "Earliest date in df_fake: 14-Feb-18\n",
      "Latest date in df_fake: https://fedup.wpengine.com/wp-content/uploads/2015/04/hillarystreetart.jpg\n",
      "\n",
      "Earliest date in df_true: April 1, 2016 \n",
      "Latest date in df_true: September 9, 2017 \n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load datasets\n",
    "df_true = pd.read_csv(\"../data/True.csv\")\n",
    "df_fake = pd.read_csv(\"../data/Fake.csv\")\n",
    "\n",
    "# Examine the shape of the dataframes\n",
    "print(\"Shape of df_fake:\", df_fake.shape)\n",
    "print(\"Shape of df_true:\", df_true.shape)\n",
    "\n",
    "# Inspect the columns\n",
    "print(\"\\nColumns of df_fake:\", df_fake.columns.values)\n",
    "print(\"Columns of df_true:\", df_true.columns.values)\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types of df_fake:\\n\", df_fake.dtypes)\n",
    "print(\"\\nData types of df_true:\\n\", df_true.dtypes)\n",
    "\n",
    "# Investigate missing values\n",
    "print(\"\\nMissing values in df_fake:\\n\", df_fake.isnull().sum())\n",
    "print(\"\\nMissing values in df_true:\\n\", df_true.isnull().sum())\n",
    "\n",
    "# Calculate descriptive statistics for numerical columns (text length)\n",
    "print(\"\\nAverage text length in df_fake:\", df_fake['text'].str.len().mean())\n",
    "print(\"Average text length in df_true:\", df_true['text'].str.len().mean())\n",
    "\n",
    "# Analyze subject distributions\n",
    "print(\"\\nSubject distribution in df_fake:\\n\", df_fake['subject'].value_counts())\n",
    "print(\"\\nSubject distribution in df_true:\\n\", df_true['subject'].value_counts())\n",
    "\n",
    "# Analyze date ranges (basic check)\n",
    "print(\"\\nEarliest date in df_fake:\", df_fake['date'].min())\n",
    "print(\"Latest date in df_fake:\", df_fake['date'].max())\n",
    "print(\"\\nEarliest date in df_true:\", df_true['date'].min())\n",
    "print(\"Latest date in df_true:\", df_true['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63bfcdcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date range analysis after cleaning:\n",
      "Earliest date in df_fake_cleaned: 14-Feb-18\n",
      "Latest date in df_fake_cleaned: https://fedup.wpengine.com/wp-content/uploads/2015/04/hillarystreetart.jpg\n",
      "\n",
      "Null values in 'date' column after cleaning:\n",
      "df_fake_cleaned: 0\n",
      "df_true: 0\n",
      "\n",
      "Number of duplicated rows in df_fake_cleaned: 0\n",
      "Number of duplicated rows in df_true: 0\n",
      "\n",
      "Empty strings in 'text' column:\n",
      "df_fake_cleaned: 0\n",
      "df_true: 0\n",
      "\n",
      "Shape of df_fake: (22848, 4)\n",
      "Shape of df_true: (21210, 4)\n"
     ]
    }
   ],
   "source": [
    "# Handle the invalid dates in df_fake\n",
    "# Given the large number, dropping rows might introduce bias\n",
    "# Drop rows with invalid dates\n",
    "\n",
    "# Remove duplicated rows from df_fake_cleaned and keep the first occurrence\n",
    "df_fake_cleaned = df_fake.drop_duplicates(keep='first')\n",
    "\n",
    "# Remove duplicated rows from df_true and keep the first occurrence\n",
    "df_true = df_true.drop_duplicates(keep='first')\n",
    "\n",
    "# Remove rows with empty strings in the 'text' column from df_fake_cleaned\n",
    "df_fake_cleaned = df_fake_cleaned[df_fake_cleaned['text'].str.strip() != '']\n",
    "\n",
    "# Remove rows with empty strings in the 'text' column from df_true\n",
    "df_true = df_true[df_true['text'].str.strip() != '']\n",
    "\n",
    "\n",
    "# Re-check the date range and missing values\n",
    "print(\"\\nDate range analysis after cleaning:\")\n",
    "print(\"Earliest date in df_fake_cleaned:\", df_fake_cleaned['date'].min())\n",
    "print(\"Latest date in df_fake_cleaned:\", df_fake_cleaned['date'].max())\n",
    "\n",
    "print(\"\\nNull values in 'date' column after cleaning:\")\n",
    "print(\"df_fake_cleaned:\", df_fake_cleaned['date'].isnull().sum())\n",
    "print(\"df_true:\", df_true['date'].isnull().sum())\n",
    "# Explore other potential issues\n",
    "# Check for duplicated rows\n",
    "print(\"\\nNumber of duplicated rows in df_fake_cleaned:\", df_fake_cleaned.duplicated().sum())\n",
    "print(\"Number of duplicated rows in df_true:\", df_true.duplicated().sum())\n",
    "\n",
    "# Further analysis\n",
    "# Example: Check for empty strings in 'text' columns\n",
    "print(\"\\nEmpty strings in 'text' column:\")\n",
    "print(\"df_fake_cleaned:\", df_fake_cleaned['text'].str.strip().eq('').sum())\n",
    "print(\"df_true:\", df_true['text'].str.strip().eq('').sum())\n",
    "\n",
    "# Examine the shape of the dataframes\n",
    "print(\"\\nShape of df_fake:\", df_fake_cleaned.shape)\n",
    "print(\"Shape of df_true:\", df_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f5e10d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subject distribution in df_fake_cleaned:\n",
      " News               0.396096\n",
      "politics           0.281425\n",
      "left-news          0.188594\n",
      "Government News    0.065564\n",
      "US_News            0.034270\n",
      "Middle-east        0.034051\n",
      "Name: subject, dtype: float64\n",
      "\n",
      "Subject distribution in df_true:\n",
      " politicsNews    0.528949\n",
      "worldnews       0.471051\n",
      "Name: subject, dtype: float64\n",
      "\n",
      "Top keywords in fake news:\n",
      "[('the', 525499), ('to', 288545), ('of', 235142), ('and', 222323), ('a', 209585), ('in', 162835), ('that', 144891), ('s', 128330), ('is', 107714), ('for', 91063)]\n",
      "\n",
      "Top keywords in true news:\n",
      "[('the', 471986), ('to', 241454), ('of', 202253), ('a', 194234), ('and', 178653), ('in', 177291), ('on', 106376), ('that', 83948), ('for', 78206), ('said', 71167)]\n"
     ]
    }
   ],
   "source": [
    "# Analyze subject distributions after cleaning df_fake\n",
    "print(\"\\nSubject distribution in df_fake_cleaned:\\n\", df_fake_cleaned['subject'].value_counts(normalize=True))\n",
    "print(\"\\nSubject distribution in df_true:\\n\", df_true['subject'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Function to find top keywords\n",
    "def find_top_keywords(df, top_n=10):\n",
    "    all_words = []\n",
    "    for text in df['text']:\n",
    "      all_words.extend(text.lower().split())\n",
    "    word_counts = Counter(all_words)\n",
    "    return word_counts.most_common(top_n)\n",
    "\n",
    "\n",
    "# Find top keywords in each dataset\n",
    "top_keywords_fake = find_top_keywords(df_fake_cleaned)\n",
    "top_keywords_true = find_top_keywords(df_true)\n",
    "\n",
    "print(\"\\nTop keywords in fake news:\")\n",
    "print(top_keywords_fake)\n",
    "\n",
    "print(\"\\nTop keywords in true news:\")\n",
    "print(top_keywords_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake_cleaned.to_csv('../data/fake_cleaned.csv', index=False)  # Save to a new file\n",
    "df_true.to_csv('../data/true_cleaned.csv', index=False)  # Save to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c07d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_true_df = pd.read_csv(\"fake_cleaned.csv\")\n",
    "# new_fake_df = pd.read_csv(\"true_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
